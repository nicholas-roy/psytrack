{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psytrack Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to paper: http://pillowlab.princeton.edu/pubs/Roy18_NeurIPS_dynamicPsychophys.pdf\n",
    "\n",
    "Last updated: March 5, 2025\n",
    "\n",
    "Psytrack version: 2.0.2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T04:03:21.040276Z",
     "start_time": "2020-04-19T04:03:20.188675Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 140\n",
    "\n",
    "import psytrack as psy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Demonstration\n",
    "\n",
    "This is a quick, 2 minute _demonstration_ of Psytrack with a simulated dataset.\n",
    "\n",
    "\\*\\* **Please find a much more in-depth tutorial using real data below** \\*\\*\n",
    "\n",
    "---\n",
    "\n",
    "### Generate simulated data\n",
    "\n",
    "This includes generating psychometric weights ${\\bf w}$, as well as the inputs ${\\bf x}$ (sampled from a standard normal) and choices ${\\bf y}$ (sampled according to the model using the weights and inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T04:03:26.696595Z",
     "start_time": "2020-04-19T04:03:26.404832Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 31\n",
    "num_weights = 4\n",
    "num_trials = 5000\n",
    "hyper = {'sigma'   : 2**np.array([-4.0,-5.0,-6.0,-7.0]),\n",
    "         'sigInit' : 2**np.array([ 0.0, 0.0, 0.0, 0.0])}\n",
    "\n",
    "# Simulate\n",
    "simData = psy.generateSim(K=num_weights, N=num_trials, hyper=hyper,\n",
    "                          boundary=6.0, iterations=1, seed=seed, savePath=None)\n",
    "\n",
    "# Plot\n",
    "psy.plot_weights(simData['W'].T);\n",
    "plt.ylim(-3.6,3.6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recover the weights from the simulated behavior\n",
    "\n",
    "That is, given inputs ${\\bf x}$ and choices ${\\bf y}$, recover the psychometric weights ${\\bf w}$.\n",
    "\n",
    "_Note: This takes approximately 60 seconds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-19T04:03:27.898Z"
    }
   },
   "outputs": [],
   "source": [
    "rec = psy.recoverSim(simData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the true weights from above (now in black) with the recovered weights (in color)\n",
    "\n",
    "Recovered weights also have shading to indicate a 95\\% credible interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-19T04:03:28.799Z"
    }
   },
   "outputs": [],
   "source": [
    "psy.plot_weights(rec['wMode'], errorbar=rec[\"hess_info\"][\"W_std\"])\n",
    "plt.plot(simData['W'], c=\"k\", ls=\"-\", alpha=0.5, lw=0.75, zorder=0)\n",
    "plt.ylim(-3.6,3.6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot recovered smoothness hyperparameters $\\sigma_k$ over the true hyperparameters (black lines)\n",
    "\n",
    "Recovered hyperparameters plotted with $\\pm$2SE bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-19T04:03:29.553Z"
    }
   },
   "outputs": [],
   "source": [
    "true_sigma = np.log2(rec['input']['sigma'])\n",
    "avg_sigma = np.log2(rec['hyp']['sigma'])\n",
    "err_sigma = rec['hess_info']['hyp_std']\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "colors = np.unique(list(psy.COLORS.values()))\n",
    "for i in range(num_weights):\n",
    "    plt.plot(i, true_sigma[i], color=\"black\", marker=\"_\", markersize=12, zorder=0)\n",
    "    plt.errorbar([i], avg_sigma[i], yerr=2*err_sigma[i], color=colors[i], lw=1, marker='o', markersize=5)\n",
    "\n",
    "plt.xticks([0,1,2,3]); plt.yticks(np.arange(-8,-2))\n",
    "plt.gca().set_xticklabels([r\"$\\sigma_1$\", r\"$\\sigma_2$\", r\"$\\sigma_3$\", r\"$\\sigma_4$\"])\n",
    "plt.xlim(-0.5,3.5); plt.ylim(-7.5,-3.5)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.ylabel(r\"$\\log_2(\\sigma)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# Full Tutorial with Real Data\n",
    "\n",
    "Datasets handled by Psytrack are specific to an individual animal and are stored as a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-19T04:03:30.461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract premade dataset from npz\n",
    "D = np.load('sampleRatData.npz', allow_pickle=True)['D'].item()\n",
    "\n",
    "print(\"The keys of the dict for this example animal:\\n   \", list(D.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these keys, only `y` and `inputs` are **required** for analysis of the dataset, all others are *optional*.\n",
    "\n",
    "---\n",
    "\n",
    "`y` should be a 1D array of the animal's choice on each trial. Currently, the analysis only works for two-alternative forced choice tasks, and so there should only be two options on each trial (error or omission trials are typically discarded from the analysis).\n",
    "\n",
    "The two options (A or B, Left or Right, etc.)  must be mapped to {1, 2} _or_ {0, 1} in `y` (not -1 and +1). In this example, Left=1 and Right=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-19T04:03:31.368Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The shape of y:   \", D['y'].shape)\n",
    "print(\"The number of trials:   N =\", D['y'].shape[0])\n",
    "print(\"The unique entries of y:   \", np.unique(D['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`inputs` is itself another dictionary, containing arbitrary keys. Each of these keys represents a _potential_ input into the model and must be a 2D array of shape $(N, M)$ where $N$ is the number of trials. The number of columns $M$ is arbitrary, and the $i^{th}$ column is typically used to encode information from $i$ time steps previous.\n",
    "\n",
    "For example, in our example data set the key `s1` encodes the (normalized) stimulus values heard on each trial. `s1[7,0]` would encode the stimulus heard on the 7th trial where as both `s1[6,0]` and `s1[7,1]` would encode the stimulus heard on the 6th trial. The information is redundant, but allows for all feasible regressors to predicting behavior of trial $i$ to be accessible by referencing the $i^{th}$ row of the respective input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-19T04:03:32.210Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The keys of inputs:\\n   \", list(D['inputs'].keys()))\n",
    "\n",
    "print(\"\\nThe shape of s1:\", D['inputs']['s1'].shape)\n",
    "print(\"s1[7]   : \", D['inputs']['s1'][7])\n",
    "print(\"s1[6,0] : \", D['inputs']['s1'][6,0])\n",
    "print(\"s1[7,1] : \", D['inputs']['s1'][7,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T20:37:02.844478Z",
     "start_time": "2019-04-01T20:37:02.806666Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "Other keys are for convenience and are _optional_ : `name` stores the name of the animal, `answer` is an easy reference as to what the correct choice was on a given trial, and `correct` is an easy reference as to whether the animal made the correct choice on a given trial. The model only needs to know what the animal _actually_ did, not what the animal _ought_ to have done!\n",
    "\n",
    "`dayLength` is an array storing the number of trials that occurred in each session of training. Taking a cumulative sum will give you the indices at which each new session begins. This is **not** optional for the analysis if one wishes to use the `sigmaDay` functionality (see Section 3.3 in paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fitting the data\n",
    "\n",
    "Once you have your data arranged in the proper format, you can now run the analysis! \n",
    "\n",
    "The fitting function is called `hyperOpt()` and before using it, you must decide on 3 inputs:\n",
    "\n",
    "   1) `weights` : which of your inputs should you fit.\n",
    "\n",
    "   2) `hyper` : what hyperparameters should your model have and how should they be initialized.\n",
    "\n",
    "   3) `optList` : what subset of the hyperparameters should be optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weights` is a dictionary where the keys correspond to the keys in your dataset's `inputs` dictionary; the key values are a non-negative integer indicating how many of the columns of that value in `inputs` should be used for fitting the model, where each included column corresponds to a new weight. You can also include in `weights` the special key `bias` which need not be included in `inputs` --- this will simply create an input of all 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'bias': 1,  # a special key\n",
    "           's1': 1,    # use only the first column of s1 from inputs\n",
    "           's2': 1}    # use only the first column of s2 from inputs\n",
    "\n",
    "# It is often useful to have the total number of weights K in your model\n",
    "K = np.sum([weights[i] for i in weights.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hyper` is a dictionary that indicates what hyperparameters your model will have. There are 3 types:\n",
    "\n",
    "1) `sigma` : required, controls trial-to-trial variability.\n",
    "\n",
    "2) `sigInit` : optional, controls the variability on the very first trial (e.g. how close weights must initialize to 0). It is often best to include this hyperparameter and set it to a high value, as you often prefer your data to determine where the weights ought to initialize. Otherwise, `sigInit` will be set equal to `sigma`.\n",
    "\n",
    "3) `sigDay` : optional, controls variability between sessions (e.g. between the last trial of one session and the first trial of the next session). If this key is included, then your dataset must also have the key `dayLength` (as described above), to indicate the trials where the `sigDay` variability should supercede the standard `sigma` variability.\n",
    "\n",
    "For each hyperparameter key included in the `hyper` dictionary, the corresponding value is the initial value of the hyperparameter. If you are optimizing over a particular hyperparameter (see the `optList` setting below), than the initial value is not so important as the fitting procedure will eventually converge to the optimal setting. However, if you are *not* optimizing, then the initial value set will be the *fixed* value of the hyperparameter.\n",
    "\n",
    "Finally, for each hyperparameter key in `hyper`, you must specify your initializations as a 1D array with length $K$. If you instead provide only a single value, then the optimizer will assume that you want the same hyperparameter to apply to every weight (as opposed to each weight having it's own)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper= {'sigInit': 2**4.,      # Set to a single, large value for all weights. Will not be optimized further.\n",
    "        'sigma': [2**-4.]*K,   # Each weight will have it's own sigma optimized, but all are initialized the same\n",
    "        'sigDay': None}        # Indicates that session boundaries will be ignored in the optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optList` is a list of the subset of hyperparameters in `hyper` that you wish to optimize over in your model. It is typically unnecessary to optimize over `sigInit` -- a single, large, fixed value is usually best. Thus, there are 4 reasonable ways to specify `optList`:\n",
    "\n",
    "1) `optList = []` : this means that none of the hyperparameters in `hyper` will be optimized. The optimizer will find the best set of weight trajectories given the fixed hyperparameter values specified.\n",
    "\n",
    "2) `optList = ['sigma']` : only the `sigma` hyperparameter will be optimized (as we will do below). If `sigDay` is included in `hyper`, then this means that the model _will_ include `sigDay`, but that the initial value specified will not be optimized.\n",
    "\n",
    "3) `optList = ['sigDay']` : only the `sigDay` hyperparameter will be optimized, while the `sigma` hyperparameter remains fixed.\n",
    "\n",
    "4) `optList = ['sigma', 'sigDay']` : both the `sigma` and `sigDay` hyperparameters will be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optList = ['sigma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have specified `weights`, `hyper`, and `optList`, we can fit our model with `hyperOpt()`! The function takes your dataset `D` plus the three additional inputs, and returns 4 things:\n",
    "\n",
    "1) `hyp` : a dictionary of the optimized hyperparameters\n",
    "\n",
    "2) `evd` : the approximate log-evidence of the optimized model\n",
    "\n",
    "3) `wMode` : the weight trajectories of the optimized model\n",
    "\n",
    "4) `hess_info` : a dictionary of sparse terms that relate to the Hessian of the optimal model. By default, this also includes the posterior credible intervals on the weights, under the key `W_std`. This behavior can be altered by changing the optional argument `hess_calc` in `hyperOpt()` (see function documentation for more details).\n",
    "\n",
    "Run times will depend on the number of trials $N$ and weights $K$, as well as the number of hyperparameters being fit. To speed things up a bit, we will use the `trim` function to shrink our dataset of 20K trials to just the first 10K.\n",
    "\n",
    "_Note: this should take < 60 seconds._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_D = psy.trim(D, END=10000)  # trim dataset to first 10,000 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp, evd, wMode, hess_info = psy.hyperOpt(new_D, hyper, weights, optList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T17:57:51.009604Z",
     "start_time": "2019-04-02T17:57:10.783614Z"
    }
   },
   "source": [
    "---\n",
    "### Visualizing the results\n",
    "\n",
    "Psytrack includes a few plotting functions for visualizing the results of the fit. To see the weight trajectories, use `plot_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = psy.plot_weights(wMode, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding credible intervals on the weights can be done with the `errorbar` keyword argument, and vertical lines indicating session boundaries can be added with `days`. Adjustments to the resulting plot can be made by editing the figure returned by the function directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = psy.plot_weights(wMode, weights, days=new_D[\"dayLength\"], errorbar=hess_info[\"W_std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T18:16:01.617268Z",
     "start_time": "2019-04-02T18:16:01.583236Z"
    }
   },
   "source": [
    "We can also generate two additional plots with useful information: \n",
    "\n",
    "1) A performance plot, tracking the animal's task accuracy, smoothed over trials\n",
    "\n",
    "2) A bias plot, tracking the animal's choice bias, smoothed over trials\n",
    "\n",
    "Both of these plots will calculate their respective values directly from the data, with 2SD error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_perf = psy.plot_performance(new_D)\n",
    "fig_bias = psy.plot_bias(new_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Cross-validation\n",
    "\n",
    "The performance and bias plots above use the data directly to plot the *empirical* acuracy and bias of the animal. We can use our model to make predictions about the animal's accuracy and bias using the weight trajectories found by the model, to see if there is agreement. However, to make true predictions, we need to make predictions on trials that were held-out from the fitting procedure.\n",
    "\n",
    "We can do this using the built-in cross-validation functions `crossValidate`. This is also useful if you'd like to compare different models via cross-validated log-likelihood, rather than approximate model evidence.\n",
    "\n",
    "Similar to `hyperOpt`, `crossValidate` receives the same inputs as well an additional input `F`, which controls the number of folds in the cross-validation procedure (note that `F` must cleanly divide the number of trials in the dataset `N` -- the `trim` function can be used to make this work). `crossValidate` will then divide the dataset into `F` training and testing datasets, fitting each of the `F` training data sets. It will return `xval_logli`, the total cross-validated log-likelihood of the `F` test sets, as well `xval_pL`, the cross-validated $P(y=0)$ for each trial.\n",
    "\n",
    "_Note: Since we're fitting `F` models, this can be fairly time consuming. This particular example should take about 10 minutes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval_logli, xval_pL = psy.crossValidate(new_D, hyper, weights, optList, F=10, seed=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cross-validated prediction that the animal will go Left on each trial, `xval_pL`, we can overlay the plots of the empirical performance and bias above with a line showing the model's prediction. Fortunately, we see a close agreement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_perf_xval = psy.plot_performance(new_D, xval_pL=xval_pL)\n",
    "fig_bias_xval = psy.plot_bias(new_D, xval_pL=xval_pL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Tutorial\n",
    "\n",
    "Please post any questions to the github, and thanks for using Psytrack!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
